{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Put many sentences to the class object\n",
    "2. Tokenize the sentences to the words\n",
    "3. FreqDist this words\n",
    "4. Calculate the tf and idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Collection:\n",
    "    def __init__(self,sentences):\n",
    "        self.words_list = [self.tokenize(sentence) for sentence in sentences]\n",
    "        self.freq_dict = [self._freqdist(words) for words in self.words_list]\n",
    "\n",
    "    def tokenize(self,sentence):\n",
    "        # remove the punctuation\n",
    "        remove_punctuation_map = dict((ord(char),None) for char in string.punctuation)\n",
    "        sentence_no_punctuation = sentence.translate(remove_punctuation_map)\n",
    "        \n",
    "        # lower\n",
    "        sentence_no_punctuation = sentence_no_punctuation.lower()\n",
    "        \n",
    "        # word_tokenize\n",
    "        words = nltk.word_tokenize(sentence_no_punctuation)\n",
    "        \n",
    "        # remove stopwords\n",
    "        from nltk.corpus import stopwords\n",
    "        filtered_words = [word for word in words if word not in stopwords.words('english')]\n",
    "        \n",
    "        # stem\n",
    "        from nltk.stem import SnowballStemmer\n",
    "        snowball_stemmer = SnowballStemmer(\"english\")\n",
    "        words_stemed = [snowball_stemmer.stem(word) for word in filtered_words]\n",
    "        \n",
    "        return words_stemed\n",
    "\n",
    "    def _freqdist(self,words):\n",
    "        from nltk import FreqDist\n",
    "        fdist = FreqDist(words)\n",
    "        standard_freq_vector = fdist.most_common(50)\n",
    "        return dict(standard_freq_vector)\n",
    "\n",
    "    def tf(self, word, sentence):\n",
    "        '''\n",
    "            Calculates the number of times the word appears in the sentence\n",
    "        '''\n",
    "        word = self.tokenize(word)[0]\n",
    "        words = self.tokenize(sentence)\n",
    "        return (sum(1 for word1 in words if word1==word))/len(words)\n",
    "\n",
    "    def idf(self, word):\n",
    "        word = self.tokenize(word)[0]\n",
    "        import math\n",
    "        try:\n",
    "            return math.log(len(self.words_list)/(1+sum(1 for words in self.words_list if word in words)))\n",
    "        except ValueError:\n",
    "            return 0\n",
    "\n",
    "    def tf_idf(self,word,sentence):\n",
    "        return self.tf(word,sentence)*self.idf(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"Natural language processing (NLP) is a field of computer science, artificial intelligence and computational linguistics concerned with the interactions between computers and human (natural) languages, and, in particular, concerned with programming computers to fruitfully process large natural language corpora. Challenges in natural language processing frequently involve natural language understanding, natural language generation (frequently from formal, machine-readable logical forms), connecting language and machine perception, managing human-computer dialog systems, or some combination thereof.\"\n",
    "\n",
    "text2 = \"The Georgetown experiment in 1954 involved fully automatic translation of more than sixty Russian sentences into English. The authors claimed that within three or five years, machine translation would be a solved problem.[2] However, real progress was much slower, and after the ALPAC report in 1966, which found that ten-year-long research had failed to fulfill the expectations, funding for machine translation was dramatically reduced. Little further research in machine translation was conducted until the late 1980s, when the first statistical machine translation systems were developed.\"\n",
    "\n",
    "text3 = \"During the 1970s, many programmers began to write conceptual ontologies, which structured real-world information into computer-understandable data. Examples are MARGIE (Schank, 1975), SAM (Cullingford, 1978), PAM (Wilensky, 1978), TaleSpin (Meehan, 1976), QUALM (Lehnert, 1977), Politics (Carbonell, 1979), and Plot Units (Lehnert 1981). During this time, many chatterbots were written including PARRY, Racter, and Jabberwacky。\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['natur', 'languag', 'process', 'nlp', 'field', 'comput', 'scienc', 'artifici', 'intellig', 'comput', 'linguist', 'concern', 'interact', 'comput', 'human', 'natur', 'languag', 'particular', 'concern', 'program', 'comput', 'fruit', 'process', 'larg', 'natur', 'languag', 'corpora', 'challeng', 'natur', 'languag', 'process', 'frequent', 'involv', 'natur', 'languag', 'understand', 'natur', 'languag', 'generat', 'frequent', 'formal', 'machineread', 'logic', 'form', 'connect', 'languag', 'machin', 'percept', 'manag', 'humancomput', 'dialog', 'system', 'combin', 'thereof'], ['georgetown', 'experi', '1954', 'involv', 'fulli', 'automat', 'translat', 'sixti', 'russian', 'sentenc', 'english', 'author', 'claim', 'within', 'three', 'five', 'year', 'machin', 'translat', 'would', 'solv', 'problem2', 'howev', 'real', 'progress', 'much', 'slower', 'alpac', 'report', '1966', 'found', 'tenyearlong', 'research', 'fail', 'fulfil', 'expect', 'fund', 'machin', 'translat', 'dramat', 'reduc', 'littl', 'research', 'machin', 'translat', 'conduct', 'late', '1980s', 'first', 'statist', 'machin', 'translat', 'system', 'develop'], ['1970s', 'mani', 'programm', 'began', 'write', 'conceptu', 'ontolog', 'structur', 'realworld', 'inform', 'computerunderstand', 'data', 'exampl', 'margi', 'schank', '1975', 'sam', 'cullingford', '1978', 'pam', 'wilenski', '1978', 'talespin', 'meehan', '1976', 'qualm', 'lehnert', '1977', 'polit', 'carbonel', '1979', 'plot', 'unit', 'lehnert', '1981', 'time', 'mani', 'chatterbot', 'written', 'includ', 'parri', 'racter', 'jabberwacky。']]\n",
      "------------------------------\n",
      "[{'languag': 7, 'natur': 6, 'comput': 4, 'process': 3, 'concern': 2, 'frequent': 2, 'nlp': 1, 'field': 1, 'scienc': 1, 'artifici': 1, 'intellig': 1, 'linguist': 1, 'interact': 1, 'human': 1, 'particular': 1, 'program': 1, 'fruit': 1, 'larg': 1, 'corpora': 1, 'challeng': 1, 'involv': 1, 'understand': 1, 'generat': 1, 'formal': 1, 'machineread': 1, 'logic': 1, 'form': 1, 'connect': 1, 'machin': 1, 'percept': 1, 'manag': 1, 'humancomput': 1, 'dialog': 1, 'system': 1, 'combin': 1, 'thereof': 1}, {'translat': 5, 'machin': 4, 'research': 2, 'georgetown': 1, 'experi': 1, '1954': 1, 'involv': 1, 'fulli': 1, 'automat': 1, 'sixti': 1, 'russian': 1, 'sentenc': 1, 'english': 1, 'author': 1, 'claim': 1, 'within': 1, 'three': 1, 'five': 1, 'year': 1, 'would': 1, 'solv': 1, 'problem2': 1, 'howev': 1, 'real': 1, 'progress': 1, 'much': 1, 'slower': 1, 'alpac': 1, 'report': 1, '1966': 1, 'found': 1, 'tenyearlong': 1, 'fail': 1, 'fulfil': 1, 'expect': 1, 'fund': 1, 'dramat': 1, 'reduc': 1, 'littl': 1, 'conduct': 1, 'late': 1, '1980s': 1, 'first': 1, 'statist': 1, 'system': 1, 'develop': 1}, {'mani': 2, '1978': 2, 'lehnert': 2, '1970s': 1, 'programm': 1, 'began': 1, 'write': 1, 'conceptu': 1, 'ontolog': 1, 'structur': 1, 'realworld': 1, 'inform': 1, 'computerunderstand': 1, 'data': 1, 'exampl': 1, 'margi': 1, 'schank': 1, '1975': 1, 'sam': 1, 'cullingford': 1, 'pam': 1, 'wilenski': 1, 'talespin': 1, 'meehan': 1, '1976': 1, 'qualm': 1, '1977': 1, 'polit': 1, 'carbonel': 1, '1979': 1, 'plot': 1, 'unit': 1, '1981': 1, 'time': 1, 'chatterbot': 1, 'written': 1, 'includ': 1, 'parri': 1, 'racter': 1, 'jabberwacky。': 1}]\n"
     ]
    }
   ],
   "source": [
    "collection = Collection([text1,text2,text3])\n",
    "print(collection.words_list)\n",
    "print('-'*30)\n",
    "print(collection.freq_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural language processing (NLP) is a field of computer science, artificial intelligence and computational linguistics concerned with the interactions between computers and human (natural) languages, and, in particular, concerned with programming computers to fruitfully process large natural language corpora. Challenges in natural language processing frequently involve natural language understanding, natural language generation (frequently from formal, machine-readable logical forms), connecting language and machine perception, managing human-computer dialog systems, or some combination thereof.\n",
      "------------------------------\n",
      "['natur', 'languag', 'process', 'nlp', 'field', 'comput', 'scienc', 'artifici', 'intellig', 'comput', 'linguist', 'concern', 'interact', 'comput', 'human', 'natur', 'languag', 'particular', 'concern', 'program', 'comput', 'fruit', 'process', 'larg', 'natur', 'languag', 'corpora', 'challeng', 'natur', 'languag', 'process', 'frequent', 'involv', 'natur', 'languag', 'understand', 'natur', 'languag', 'generat', 'frequent', 'formal', 'machineread', 'logic', 'form', 'connect', 'languag', 'machin', 'percept', 'manag', 'humancomput', 'dialog', 'system', 'combin', 'thereof']\n"
     ]
    }
   ],
   "source": [
    "text1_tokenized = collection.tokenize(text1)\n",
    "print(text1)\n",
    "print('-'*30)\n",
    "print(text1_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1111111111111111\n"
     ]
    }
   ],
   "source": [
    "res = collection.tf('nature', text1)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05256029179179909\n"
     ]
    }
   ],
   "source": [
    "res = collection.tf_idf('language',text1)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
