{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sklearn for NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Vectorizer\n",
    "Caculates the TF matrix for each documents.\n",
    "\n",
    "Suppose our document space is listed below:\n",
    "```\n",
    "Train Document Set:\n",
    "\n",
    "d1: The sky is blue.\n",
    "d2: The sun is bright.\n",
    "\n",
    "Test Document Set:\n",
    "\n",
    "d3: The sun in the sky is bright.\n",
    "d4: We can see the shining sun, the bright sun.\n",
    "```\n",
    "\n",
    "取小写的字母,移除不必要的stopwords等操作后,我们可以建立vocabulary dictionary,如下所示:\n",
    "```\n",
    "{'blue': 0, 'bright': 1, 'sky': 2, 'sun': 3}\n",
    "```\n",
    "\n",
    "那么对于d3来说,blue出现0次,bright出现1次,sky出现1次,sun出现1次,所以d3的TF Feature就是:\n",
    "```\n",
    "[0, 1, 1, 1]\n",
    "```\n",
    "\n",
    "对于d4来说,blue出现0次,bright出现1次,sky出现0次,sun出现2次,所以d4的TF Feature就是:\n",
    "```\n",
    "[0, 1, 0, 2]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data:\n",
      "('The sky is blue.', 'The sun is bright.')\n",
      "test_data:\n",
      "('The sun in the sky is bright.', 'We can see the shining sun, the bright sun.')\n"
     ]
    }
   ],
   "source": [
    "train_data = (\"The sky is blue.\", \"The sun is bright.\")\n",
    "test_data = (\"The sun in the sky is bright.\", \"We can see the shining sun, the bright sun.\")\n",
    "print('train_data:')\n",
    "print(train_data)\n",
    "print('test_data:')\n",
    "print(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None,\n",
      "        stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs',... 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"],\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "\n",
      "{'sky': 2, 'blue': 0, 'sun': 3, 'bright': 1}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words=stopwords.words('english'))\n",
    "\n",
    "print(vectorizer)\n",
    "print()\n",
    "vectorizer.fit_transform(train_data)\n",
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CountVectorizer already uses as default “analyzer” called WordNGramAnalyzer, which is responsible to convert the text to lowercase, accents removal, token extraction, (filter stop words,) etc… you can see more information by printing the class information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 3)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 3)\t2\n"
     ]
    }
   ],
   "source": [
    "smatrix = vectorizer.transform(test_data)\n",
    "print(smatrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the sparse matrix created called smatrix is a Scipy sparse matrix with elements stored in a Coordinate format. But you can convert it into a dense format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 1]\n",
      " [0 1 0 2]]\n"
     ]
    }
   ],
   "source": [
    "tf_features = smatrix.toarray()\n",
    "print(tf_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
