{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sklearn for NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Vectorizer\n",
    "Caculates the TF matrix for each documents.\n",
    "\n",
    "Suppose our document space is listed below:\n",
    "```\n",
    "Train Document Set:\n",
    "\n",
    "d1: The sky is blue.\n",
    "d2: The sun is bright.\n",
    "\n",
    "Test Document Set:\n",
    "\n",
    "d3: The sun in the sky is bright.\n",
    "d4: We can see the shining sun, the bright sun.\n",
    "```\n",
    "\n",
    "取小写的字母,移除不必要的stopwords等操作后,我们可以建立vocabulary dictionary,如下所示:\n",
    "```\n",
    "{'blue': 0, 'bright': 1, 'sky': 2, 'sun': 3}\n",
    "```\n",
    "\n",
    "那么对于d3来说,blue出现0次,bright出现1次,sky出现1次,sun出现1次,所以d3的TF Feature就是:\n",
    "```\n",
    "[0, 1, 1, 1]\n",
    "```\n",
    "\n",
    "对于d4来说,blue出现0次,bright出现1次,sky出现0次,sun出现2次,所以d4的TF Feature就是:\n",
    "```\n",
    "[0, 1, 0, 2]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data:\n",
      "('The sky is blue.', 'The sun is bright.')\n",
      "test_data:\n",
      "('The sun in the sky is bright.', 'We can see the shining sun, the bright sun.')\n"
     ]
    }
   ],
   "source": [
    "train_data = (\"The sky is blue.\", \"The sun is bright.\")\n",
    "test_data = (\"The sun in the sky is bright.\", \"We can see the shining sun, the bright sun.\")\n",
    "print('train_data:')\n",
    "print(train_data)\n",
    "print('test_data:')\n",
    "print(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None,\n",
      "        stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs',... 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"],\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "\n",
      "{'sky': 2, 'blue': 0, 'sun': 3, 'bright': 1}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words=stopwords.words('english'))\n",
    "\n",
    "print(vectorizer)\n",
    "print()\n",
    "vectorizer.fit_transform(train_data)\n",
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CountVectorizer already uses as default “analyzer” called WordNGramAnalyzer, which is responsible to convert the text to lowercase, accents removal, token extraction, (filter stop words,) etc… you can see more information by printing the class information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 3)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 3)\t2\n"
     ]
    }
   ],
   "source": [
    "smatrix = vectorizer.transform(test_data)\n",
    "print(smatrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the sparse matrix created called smatrix is a Scipy sparse matrix with elements stored in a Coordinate format. But you can convert it into a dense format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 1]\n",
      " [0 1 0 2]]\n"
     ]
    }
   ],
   "source": [
    "tf_features = smatrix.toarray()\n",
    "print(tf_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TfidfVectorizer\n",
    "\n",
    "Tf means term-frequency while tf–idf means term-frequency times inverse document-frequency:\n",
    "\n",
    "$\\text{tf-idf(t, d, D)}=\\text{tf(t, d)} \\times \\text{idf(t, D)}$.\n",
    "\n",
    "\n",
    "\n",
    "Using the `TfidfTransformer`’s default settings, `TfidfTransformer(norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=False)` the term frequency, the number of times a term occurs in a given document, is multiplied with idf component, which is computed as\n",
    "\n",
    "$\\text{idf}(t, D) = \\log{\\frac{1 + n}{1+\\text{df}(t)}} + 1$,\n",
    "\n",
    "where $N$ is the total number of documents and $df(t)$ is the number of documents that contains token $t$. \n",
    "\n",
    "$\\text{tf(t, d)}$ is the number of times token $t$ appears in the document $d$.\n",
    "\n",
    "The resulting tf-idf vectors are then normalized by the Euclidean norm:\n",
    "\n",
    "$v_{norm} = \\frac{v}{||v||_2} = \\frac{v}{\\sqrt{v{_1}^2 +\n",
    "v{_2}^2 + \\dots + v{_n}^2}}$\n",
    "\n",
    "See details in [this](https://scikit-learn.org/stable/modules/feature_extraction.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test data. Both the full documents and their labels (\"Sports\" vs \"Non Sports\")\n",
    "train_data = ['Football: a great sport', 'The referee has been very bad this season', 'Our team scored 5 goals', 'I love tennis',\n",
    "              'Politics is in decline in the UK', 'Brexit means Brexit', 'The parlament wants to create new legislation',\n",
    "              'I so want to travel the world']\n",
    "train_labels = [\"Sports\",\"Sports\",\"Sports\",\"Sports\", \"Non Sports\", \"Non Sports\", \"Non Sports\", \"Non Sports\"]\n",
    "\n",
    "test_data = ['Swimming is a great sport', \n",
    "             'A lot of policy changes will happen after Brexit', \n",
    "             'The table tennis team will travel to the UK soon for the European Championship']\n",
    "test_labels = [\"Sports\",\"Non Sports\",\"Sports\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case: TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs',... 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"],\n",
      "        strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None)\n",
      "{'football': 4, 'great': 6, 'sport': 16, 'referee': 13, 'bad': 0, 'season': 15, 'team': 17, 'scored': 14, 'goals': 5, 'love': 8, 'tennis': 18, 'politics': 12, 'decline': 3, 'uk': 20, 'brexit': 1, 'means': 9, 'parlament': 11, 'wants': 22, 'create': 2, 'new': 10, 'legislation': 7, 'want': 21, 'travel': 19, 'world': 23}\n",
      "24\n",
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.70710678 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.70710678 0.\n",
      "  0.         0.         0.         0.         0.         0.        ]\n",
      " [0.         1.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.5\n",
      "  0.5        0.5        0.5        0.         0.         0.        ]]\n",
      "(3, 24)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/orris/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:1089: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=stopwords.words('english'))\n",
    "\n",
    "print(vectorizer)\n",
    "\n",
    "vectorizer.fit_transform(train_data)\n",
    "print(vectorizer.vocabulary_)\n",
    "print(len(vectorizer.vocabulary_))\n",
    "\n",
    "smtarix = vectorizer.transform(test_data)\n",
    "test_features = smtarix.toarray()\n",
    "print(test_features)\n",
    "print(test_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case: Implement on ourselves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.70710677 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.70710677 0.\n",
      "  0.         0.         0.         0.         0.         0.        ]\n",
      " [0.         1.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.5\n",
      "  0.5        0.5        0.5        0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "def tokenize(sentence):\n",
    "    # remove the punctuation\n",
    "    import string\n",
    "    remove_punctuation_map = dict((ord(char),None) for char in string.punctuation)\n",
    "    sentence_no_punctuation = sentence.translate(remove_punctuation_map)\n",
    "        \n",
    "    # lower\n",
    "    sentence_no_punctuation = sentence_no_punctuation.lower()\n",
    "        \n",
    "    # word_tokenize\n",
    "    from nltk import word_tokenize\n",
    "    words = word_tokenize(sentence_no_punctuation)\n",
    "        \n",
    "    # remove stopwords\n",
    "    from nltk.corpus import stopwords\n",
    "    filtered_words = [word for word in words if word not in stopwords.words('english')]\n",
    "        \n",
    "    # stem\n",
    "    from nltk.stem import SnowballStemmer\n",
    "    snowball_stemmer = SnowballStemmer(\"english\")\n",
    "    words_stemed = [snowball_stemmer.stem(word) for word in filtered_words]\n",
    "    \n",
    "    return words_stemed\n",
    "\n",
    "def tf(word, document):\n",
    "    word = tokenize(word)[0]\n",
    "    words = tokenize(document)\n",
    "    return sum(1 for word1 in words if word1 == word)\n",
    "\n",
    "def idf(word, documents):\n",
    "    tokens_list = [tokenize(document) for document in documents]\n",
    "    token = tokenize(word)[0]\n",
    "    import math\n",
    "    try:\n",
    "        return math.log((1 + len(tokens_list)) / (1 + sum(1 for tokens in tokens_list if token in tokens)))\n",
    "    except ValueError:\n",
    "        return 0\n",
    "\n",
    "def tf_idf(word, document, documents):\n",
    "    return tf(word, document) * idf(word, documents)\n",
    "\n",
    "vocabulary = vectorizer.vocabulary_\n",
    "vocab_size = len(vocabulary)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "tf_idf_features = np.zeros((len(test_data), vocab_size), np.float32)\n",
    "# computes tf-idf features\n",
    "for i in range(len(test_data)):\n",
    "    for token, index in vocabulary.items():\n",
    "        tf_idf_features[i, index] = tf_idf(token, test_data[i], test_data)\n",
    "    # normalize\n",
    "    tf_idf_features[i] = tf_idf_features[i] / np.sqrt(np.sum(tf_idf_features[i] ** 2))\n",
    "print(tf_idf_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is the same as what we obtain using sklearn TfidfVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
