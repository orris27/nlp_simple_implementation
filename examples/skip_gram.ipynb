{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Usage: Download text8.zip(http://mattmahoney.net/dc/text8.zip) and `python skip-gram.py`\n",
    "    Training Dataset: \n",
    "        features: [batch_size] (indices)\n",
    "        labels: [batch_size, 1] (indices)\n",
    "    Graph: skip-gram.png\n",
    "'''\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "import collections\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# Constants\n",
    "###############################################################################\n",
    "\n",
    "filename = \"text8.zip\"\n",
    "\n",
    "#vocab_size = 5000\n",
    "vocab_size = 50000\n",
    "skip_window = 1\n",
    "num_skips = 2\n",
    "\n",
    "#batch_size = 8\n",
    "batch_size = 128\n",
    "\n",
    "#embedding_size = 1000\n",
    "embedding_size = 128\n",
    "\n",
    "num_sampled = 64\n",
    "learning_rate = 0.01\n",
    "#num_epochs = 10001\n",
    "num_epochs = 100001\n",
    "log_every = 100\n",
    "\n",
    "valid_size = 20\n",
    "valid_window = 100\n",
    "top_k = 10\n",
    "\n",
    "\n",
    "picname = \"my-pic.png\"\n",
    "\n",
    "###############################################################################\n",
    "# Build dataset: generate_batch\n",
    "###############################################################################\n",
    "\n",
    "\n",
    "# read the text8.zip into a list of words\n",
    "\n",
    "with zipfile.ZipFile(filename) as f:\n",
    "    words = (tf.compat.as_str(f.read(f.namelist()[0]))).split()\n",
    "\n",
    "    \n",
    "# transfer the list of words into a list of indices\n",
    "def words_to_indices(words):\n",
    "    # count words and filter the most common words\n",
    "    count = collections.Counter(words).most_common(vocab_size - 1)\n",
    "    # add <unk>\n",
    "    freq = [['<unk>', -1]]\n",
    "    freq.extend(count)\n",
    "    \n",
    "    dictionary = dict()\n",
    "    # calc the dictionary\n",
    "    for i in range(vocab_size):\n",
    "        dictionary[freq[i][0]] = i\n",
    "\n",
    "    # calc the reverse_dictionary\n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "\n",
    "    # define a indices\n",
    "    indices = []\n",
    "    # create indices\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            indices.append(dictionary[word])\n",
    "        else:\n",
    "            indices.append(0)\n",
    "    \n",
    "    return indices, dictionary, reverse_dictionary\n",
    "    \n",
    "indices, dictionary, reverse_dictionary = words_to_indices(words)\n",
    "\n",
    "del words # save memory\n",
    "\n",
    "data_index = 0\n",
    "\n",
    "# define generate_batch function\n",
    "def generate_batch(indices, batch_size):\n",
    "\n",
    "    global data_index\n",
    "    len_indices = len(indices)\n",
    "\n",
    "    X = np.ndarray(shape=(batch_size,), dtype=np.int32)\n",
    "    y = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "\n",
    "    dq_len = skip_window * 2 + 1\n",
    "    # create a deque with length of \"skip_window * 2 + 1\"\n",
    "    dq = collections.deque(maxlen=dq_len)\n",
    "    # init dq\n",
    "    for _ in range(dq_len):\n",
    "        dq.append(indices[data_index])\n",
    "        data_index = (data_index + 1) % len_indices\n",
    "\n",
    "    sub = 0\n",
    "    for _ in range(batch_size // num_skips):\n",
    "\n",
    "        # obtain the feature: dq[skip_window]\n",
    "        # randomly choose num_skips elms from 0 to skip_window*2+1(not included) except skip_window\n",
    "        # 1. create a list <choices>:[0, skip_window*2+1)\n",
    "        choices = [i for i in range(skip_window * 2 + 1)]\n",
    "        # 2. remove skip_window\n",
    "        choices.pop(skip_window)\n",
    "        # 3. while len(choices) != num_skips:\n",
    "        while len(choices) != num_skips:\n",
    "            # 1. randomly determine a elm: [0, len(choices))\n",
    "            choice = np.random.randint(0, len(choices))\n",
    "            # 2. remove it\n",
    "            choices.pop(choice)\n",
    "\n",
    "        # obtain the label\n",
    "        for choice in choices:\n",
    "            X[sub] = dq[skip_window]\n",
    "            y[sub][0] = dq[choice]\n",
    "            sub += 1\n",
    "\n",
    "        # move the dq\n",
    "        dq.append(indices[data_index])\n",
    "        data_index = (data_index + 1) % len_indices\n",
    "\n",
    "    return X, y\n",
    "\n",
    "###############################################################################\n",
    "# Construct a tensor graph: train + valid\n",
    "###############################################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    with tf.name_scope('placeholder'):\n",
    "        # define features placeholder\n",
    "        features = tf.placeholder(tf.int32,[batch_size], name='features') \n",
    "        # define labels placeholder\n",
    "        labels = tf.placeholder(tf.int32,[batch_size,1], name='labels')\n",
    "\n",
    "\n",
    "    # define embedding matrix\n",
    "    embedding = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0))\n",
    "    # embedding: [batch_size] => [batch_size, embedding_size]\n",
    "    embedded_chars = tf.nn.embedding_lookup(embedding, features)\n",
    "\n",
    "    with tf.name_scope(\"nce\"):\n",
    "        # define NCE loss\n",
    "        nce_weights = tf.Variable(tf.truncated_normal([vocab_size, embedding_size], stddev=1.0 / math.sqrt(embedding_size)))\n",
    "        nce_biases = tf.Variable(tf.zeros([vocab_size]))\n",
    "\n",
    "        loss = tf.reduce_mean(tf.nn.nce_loss(weights = nce_weights, # [num_classes, embedding_size]\n",
    "                                             biases = nce_biases, # [num_classes] \n",
    "                                             inputs = embedded_chars, # [batch_size, embedding_size]\n",
    "                                             labels = labels, # [batch_size, num_true]; num_true means represents the number of positive samples\n",
    "                                             num_sampled = num_sampled, # the number of negative samples\n",
    "                                             num_classes = vocab_size)) # num_classes\n",
    "\n",
    "    # train\n",
    "    #train = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "    train = tf.train.GradientDescentOptimizer(1.0).minimize(loss)\n",
    "\n",
    "    # randomly generate dataset designed to valid: [valid_size]\n",
    "    valid_features = np.random.choice(valid_window, size=(valid_size,), replace=False)\n",
    "\n",
    "    # define unitized embedding matrix\n",
    "    def unitize_2d(vectors):\n",
    "        return vectors / tf.sqrt(tf.reduce_sum(tf.square(vectors), axis=1, keepdims=True))\n",
    "\n",
    "    unitized_embedding = unitize_2d(embedding)\n",
    "\n",
    "    # embedding: [valid_size] => [valid_size, embedding_size]\n",
    "    valid_embedded_chars = tf.nn.embedding_lookup(unitized_embedding, valid_features)\n",
    "\n",
    "    # NN using unitized embedding matrix: [valid_size, embedding_size] => ([embedding_size, vocab_size]) => [valid_size, vocab_size]\n",
    "    final_embedding = tf.matmul(valid_embedded_chars, unitized_embedding, transpose_b=True)\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# Train + valid(print)\n",
    "###############################################################################\n",
    "\n",
    "\n",
    "\n",
    "# check for accuracy\n",
    "# words: [valid_size, vocab_size]\n",
    "# labels: [valid_size]. contains the indices of these words\n",
    "def check(words, labels):\n",
    "    # sort the dim1 reversely\n",
    "    res = []\n",
    "    # pick out the first <top_k> for each word\n",
    "    for word in words:\n",
    "        res.append(((-word).argsort())[:top_k])\n",
    "    \n",
    "    # print dictionary[word] & dictionary[top_k_word]\n",
    "    for i, label in enumerate(labels):\n",
    "        print(\"Nearest to\", reverse_dictionary[label],end=': ')\n",
    "        for j in range(top_k):\n",
    "            print(reverse_dictionary[res[i][j]], end=' ')\n",
    "        print()\n",
    "\n",
    "\n",
    "gpu_options=tf.GPUOptions(per_process_gpu_memory_fraction=0.7)\n",
    "config=tf.ConfigProto(gpu_options=gpu_options)\n",
    "# with tf.Session() as sess:\n",
    "with tf.Session(config=config, graph=graph) as sess:\n",
    "    # init global variables\n",
    "    tf.global_variables_initializer().run()\n",
    "\n",
    "\n",
    "    # define a loss to recv\n",
    "    total_loss = 0.\n",
    "\n",
    "    num_batches = len(indices) // batch_size\n",
    "\n",
    "    #print(\"Start {0} epochs' training...\".format(num_epochs * num_batches))\n",
    "\n",
    "\n",
    "    # for i in range(num_epochs * num_batches):\n",
    "    #for step in range(num_epochs * num_batches):\n",
    "    for step in range(num_epochs):\n",
    "        # get X,y from generate_batch\n",
    "        X, y = generate_batch(indices, batch_size)\n",
    "        # train and get NCE loss\n",
    "        _, loss1 = sess.run([train, loss], feed_dict = {features:X, labels:y})\n",
    "\n",
    "        total_loss += loss1\n",
    "\n",
    "        # if step % LOG_EVERY == 0:\n",
    "        if step % log_every == 0:\n",
    "            # print NCE loss\n",
    "            print(\"step\", step, \": loss =\" ,total_loss / (step + 1))\n",
    "\n",
    "    # get final embedding matrix\n",
    "    final_embedding1 = sess.run(final_embedding)\n",
    "    \n",
    "    # check for accuracy\n",
    "    check(final_embedding1, valid_features)\n",
    "\n",
    "    # TSNE\n",
    "    from sklearn.manifold import TSNE\n",
    "    tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=5000)\n",
    "\n",
    "    unitized_embedding1 = sess.run(unitized_embedding)\n",
    "\n",
    "    plot_only = 100\n",
    "    low_dim_embs = tsne.fit_transform(unitized_embedding1[:plot_only])\n",
    "\n",
    "    # get indices, translate\n",
    "    annotations = []\n",
    "    for i in range(plot_only):\n",
    "        annotations.append(reverse_dictionary[i])\n",
    "    \n",
    "    # draw a graph that describes the embedding matrix\n",
    "    def plot_with_labels(low_dim_embs, annotations):\n",
    "        plt.figure(figsize=(18,18))\n",
    "        for i, annotation in enumerate(annotations):\n",
    "            x, y = low_dim_embs[i, :]\n",
    "            plt.scatter(x, y)\n",
    "            plt.annotate(annotation,\n",
    "                         xy = (x, y),\n",
    "                         xytext = (5, 2),\n",
    "                         textcoords = 'offset points',\n",
    "                         ha = 'right',\n",
    "                         va = 'bottom')\n",
    "\n",
    "        plt.savefig(picname)\n",
    "\n",
    "    plot_with_labels(low_dim_embs,annotations)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
