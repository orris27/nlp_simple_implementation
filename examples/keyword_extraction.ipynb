{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keyword Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import jieba.posseg as psg\n",
    "from operator import itemgetter\n",
    "from jieba import analyse\n",
    "\n",
    "def load_stopwords(stopword_filename='./data/stopword.txt'):\n",
    "    with open(stopword_filename, 'r', encoding='utf8') as f:\n",
    "        lines = f.readlines()\n",
    "        lines = [line.strip() for line in lines]\n",
    "    return lines\n",
    "\n",
    "def tokenize(document):\n",
    "    tokens = list()\n",
    "    # tokenize & remove nouns & remove token with length less than 2\n",
    "    for token, tag in psg.cut(document):\n",
    "        if tag.startswith('n') and len(token) >= 2:\n",
    "            tokens.append(token)\n",
    "    stopwords = load_stopwords()\n",
    "    # remove stopwords\n",
    "    tokens = [token for token in tokens if token not in stopwords]\n",
    "    return tokens\n",
    "\n",
    "def tf(word, document):\n",
    "    word = tokenize(word)[0]\n",
    "    words = tokenize(document)\n",
    "    return (sum(1 for word1 in words if word1 == word)) / len(words)\n",
    "\n",
    "def idf(word, tokens_list):\n",
    "    \n",
    "    token = tokenize(word)[0]\n",
    "    import math\n",
    "    try:\n",
    "        return math.log(len(tokens_list) / (1 + sum(1 for tokens in tokens_list if token in tokens)))\n",
    "    except ValueError:\n",
    "        return 0\n",
    "\n",
    "def tf_idf(word, document, tokens_list):\n",
    "    return tf(word, document) * idf(word, tokens_list)\n",
    "\n",
    "def load_data(corpus_filename='./data/corpus.txt'):\n",
    "    documents = list()\n",
    "    with open(corpus_filename, 'r', encoding='utf8') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            line.strip()\n",
    "            documents.append(line)\n",
    "    return documents\n",
    "\n",
    "def extract_keyword_tf_idf(text, num_keywords):\n",
    "    documents = load_data()\n",
    "    tokens_list = [tokenize(document) for document in documents]\n",
    "    stopwords = load_stopwords()\n",
    "    tokens = tokenize(text)\n",
    "    tokens = set(tokens)\n",
    "    tf_idfs = {}\n",
    "    for token in tokens:\n",
    "        tf_idfs[token] = tf_idf(token, text, tokens_list)\n",
    "    keyword_tf_idfs = sorted(tf_idfs.items(), key=itemgetter(1), reverse=True)\n",
    "    keywords = [keyword for keyword, tf_idf in keyword_tf_idfs]\n",
    "    keywords = keywords[:num_keywords]\n",
    "    return keywords\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_textrank1(text, num_keywords, allowPOS=('ns', 'n', 'vn', 'v'), window_size=5):\n",
    "    d = 0.85\n",
    "    keywords = list()\n",
    "    tokens = tokenize(text)\n",
    "    in_set = {token: set() for token in tokens}\n",
    "    out_set = {token: set() for token in tokens}\n",
    "\n",
    "    for i in range(len(tokens) - window_size):\n",
    "        sub_tokens = tokens[i: i + window_size]\n",
    "        for token in sub_tokens:\n",
    "            for other_token in sub_tokens:\n",
    "                if other_token != token:\n",
    "                    in_set[token] |= {other_token}\n",
    "                    out_set[other_token] |= {token}\n",
    "    scores = {token: 1.0 for token in tokens}\n",
    "    tmp_scores = {token: 0.0 for token in tokens}\n",
    "    #d = \n",
    "    for _ in range(1000):\n",
    "        for token in tokens:\n",
    "            tmp_scores[token] = 0.0\n",
    "            for in_token in in_set[token]:\n",
    "                tmp_scores[token] += scores[in_token] / len(out_set[in_token])\n",
    "            tmp_scores[token] = (1 - d) + d * tmp_scores[token]\n",
    "            scores = tmp_scores\n",
    "            \n",
    "    scores = sorted(scores.items(), key=itemgetter(1), reverse=True)\n",
    "    keywords = [keyword for keyword, score in scores[:num_keywords]]\n",
    "    return keywords\n",
    "\n",
    "def my_textrank(text, num_keywords, allowPOS=('ns', 'n', 'vn', 'v'), window_size=5):\n",
    "    d = 0.85\n",
    "    keywords = list()\n",
    "    tokens = tokenize(text)\n",
    "    in_set = {token: dict() for token in tokens} # {token1: {in_token1: weight, ...}, ...}\n",
    "    out_set = {token: dict() for token in tokens}\n",
    "\n",
    "    for i in range(len(tokens) - window_size):\n",
    "        sub_tokens = tokens[i: i + window_size]\n",
    "        for token in sub_tokens:\n",
    "            for other_token in sub_tokens:\n",
    "                if other_token != token:\n",
    "                    # The number of A->B is set to be the weight\n",
    "                    in_set[token][other_token] = in_set[token].get(other_token, 0) + 1\n",
    "                    out_set[other_token][token] = out_set[other_token].get(token, 0) + 1\n",
    "                    \n",
    "    scores = {token: 1.0 for token in tokens}\n",
    "    tmp_scores = {token: 0.0 for token in tokens}\n",
    "    for _ in range(10):\n",
    "        for token in tokens:\n",
    "            tmp_scores[token] = 0.0\n",
    "            for in_token in in_set[token].keys():\n",
    "                tmp_scores[token] += scores[in_token] / len(out_set[in_token]) * in_set[token].get(other_token, 1.0)\n",
    "            tmp_scores[token] = (1 - d) + d * tmp_scores[token]\n",
    "            scores = tmp_scores\n",
    "            \n",
    "    scores = sorted(scores.items(), key=itemgetter(1), reverse=True)\n",
    "    keywords = [keyword for keyword, score in scores[:num_keywords]]\n",
    "    return keywords\n",
    "\n",
    "\n",
    "\n",
    "def extract_keyword_textrank(text, num_keywords):\n",
    "    keywords = list()\n",
    "    textrank = analyse.textrank\n",
    "    keywords = textrank(text, num_keywords, allowPOS=('ns', 'n'))\n",
    "    return keywords\n",
    "\n",
    "def my_extract_keyword_textrank(text, num_keywords):\n",
    "    keywords = list()\n",
    "    textrank = analyse.textrank\n",
    "    keywords = my_textrank(text, num_keywords, allowPOS=('ns', 'n'))\n",
    "    \n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "from operator import itemgetter\n",
    "import math\n",
    "\n",
    "def extract_keyword_topic(text, num_keywords, mode, num_topics=4):\n",
    "    # create BoW tf_idf\n",
    "    documents = load_data()\n",
    "    tokens_list = [tokenize(document) for document in documents]\n",
    "    \n",
    "    # construct index<->word map\n",
    "    d = corpora.Dictionary(tokens_list) \n",
    "    # Convert tokens_list to BoW format\n",
    "    bow_tokens_list = [d.doc2bow(tokens) for tokens in tokens_list]\n",
    "    # Train tfidf Model using BoW data\n",
    "    tfidf_model = models.TfidfModel(bow_tokens_list)\n",
    "    # Calculates tfidf features(BoW format) for each document using BoW data\n",
    "    bow_tfidf_features_list = [tfidf_model[bow_tokens] for bow_tokens in bow_tokens_list]\n",
    "    \n",
    "    # Train LSI/LDA model using tfidf features(BoW format)\n",
    "    if mode == 'lsi':\n",
    "        model = models.LsiModel(bow_tfidf_features_list, id2word=d, num_topics=num_topics)\n",
    "    elif mode == 'lda':\n",
    "        model = models.LsiModel(bow_tfidf_features_list, id2word=d, num_topics=num_topics)\n",
    "    else:\n",
    "        return list()\n",
    "    \n",
    "    # Get token set\n",
    "    tmp_tokens = list()\n",
    "    for tokens in tokens_list:\n",
    "        tmp_tokens.extend(tokens)\n",
    "    token_set = set(tmp_tokens)\n",
    "    \n",
    "    # Get 'token->topic feature' map\n",
    "    token_topics = dict()\n",
    "    for token in token_set:\n",
    "        single_tokens_list = [token]\n",
    "        # convert to tfidf features(BoW format)\n",
    "        bow_tfidf_feature = tfidf_model[d.doc2bow(single_tokens_list)]\n",
    "        topic = model[bow_tfidf_feature]\n",
    "        \n",
    "        token_topics[token] = topic\n",
    "    \n",
    "    # Get document topic feature\n",
    "    tokens = tokenize(text)\n",
    "    bow_tfidf_features = tfidf_model[d.doc2bow(tokens)]\n",
    "    text_topic = model[bow_tfidf_features]\n",
    "    \n",
    "    # Calculate the topic features for each word in document\n",
    "    def calculate_simlilarity(l1, l2):\n",
    "        try:\n",
    "            sim = math.sqrt(sum(topic ** 2 for topic_id, topic in l1) / sum(topic ** 2 for topic_id, topic in l2))\n",
    "            # 貌似用余弦相似度结果会比较差\n",
    "            #sim = math.sqrt(sum([(id_topic1[1] - id_topic2[1]) ** 2 for id_topic1, id_topic2 in zip(l1, l2)]))\n",
    "        except ZeroDivisionError:\n",
    "            sim = 0.0\n",
    "        return sim\n",
    "\n",
    "    sim_dict = dict()\n",
    "    for token in tokens:\n",
    "        topic = token_topics[token]\n",
    "        sim = calculate_simlilarity(topic, text_topic) # the order of 'topic' and 'text_topic' cannot be changed\n",
    "        sim_dict[token] = sim\n",
    "    keyword_topics = sorted(sim_dict.items(), key=itemgetter(1), reverse=True)\n",
    "    print('-'*30)\n",
    "    print(keyword_topics[:num_keywords])\n",
    "    print('-'*30)\n",
    "    keywords = [keyword for keyword, topic in keyword_topics[:num_keywords]]\n",
    "    \n",
    "    return keywords\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keyword(text, mode, num_keywords=10):\n",
    "    keywords = list()\n",
    "    if mode == 'tf-idf':\n",
    "        keywords = extract_keyword_tf_idf(text, num_keywords)\n",
    "    elif mode == 'textrank':\n",
    "        keywords = extract_keyword_textrank(text, num_keywords)\n",
    "    elif mode == 'mytextrank':\n",
    "        keywords = my_extract_keyword_textrank(text, num_keywords)\n",
    "    elif mode == 'lsi' or mode == 'lda':\n",
    "        keywords =extract_keyword_topic(text, num_keywords, mode=mode)\n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '6月19日,《2012年度“中国爱心城市”公益活动新闻发布会》在京举行。' + \\\n",
    "       '中华社会救助基金会理事长许嘉璐到会讲话。基金会高级顾问朱发忠,全国老龄' + \\\n",
    "       '办副主任朱勇,民政部社会救助司助理巡视员周萍,中华社会救助基金会副理事长耿志远,' + \\\n",
    "       '重庆市民政局巡视员谭明政。晋江市人大常委会主任陈健倩,以及10余个省、市、自治区民政局' + \\\n",
    "       '领导及四十多家媒体参加了发布会。中华社会救助基金会秘书长时正新介绍本年度“中国爱心城' + \\\n",
    "       '市”公益活动将以“爱心城市宣传、孤老关爱救助项目及第二届中国爱心城市大会”为主要内容,重庆市' + \\\n",
    "       '、呼和浩特市、长沙市、太原市、蚌埠市、南昌市、汕头市、沧州市、晋江市及遵化市将会积极参加' + \\\n",
    "       '这一公益活动。中国雅虎副总编张银生和凤凰网城市频道总监赵耀分别以各自媒体优势介绍了活动' + \\\n",
    "       '的宣传方案。会上,中华社会救助基金会与“第二届中国爱心城市大会”承办方晋江市签约,许嘉璐理' + \\\n",
    "       '事长接受晋江市参与“百万孤老关爱行动”向国家重点扶贫地区捐赠的价值400万元的款物。晋江市人大' + \\\n",
    "       '常委会主任陈健倩介绍了大会的筹备情况。'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.779 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['晋江市', '城市', '大会', '爱心', '中华', '基金会', '重庆市', '许嘉璐', '人大常委会', '巡视员']\n"
     ]
    }
   ],
   "source": [
    "keywords = extract_keyword(text, mode='tf-idf')\n",
    "print(keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. TextRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "textrank:\n",
      "['城市', '爱心', '中国', '社会', '基金会', '晋江市', '公益活动', '大会', '地区', '发布会']\n",
      "\n",
      "mytextrank:\n",
      "['城市', '晋江市', '爱心', '中国', '承办方', '许嘉璐', '大会', '重庆市', '内容', '理事长']\n"
     ]
    }
   ],
   "source": [
    "keywords = extract_keyword(text, mode='textrank')\n",
    "print('textrank:')\n",
    "print(keywords)\n",
    "\n",
    "keywords = extract_keyword(text, mode='mytextrank')\n",
    "print('\\nmytextrank:')\n",
    "print(keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. LSI & LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "[('中国', 1.0218805794057921), ('中华', 0.9898930199536915), ('爱心', 0.8340576057836584), ('项目', 0.6760489770421642), ('基金会', 0.6696490642548523), ('社会', 0.6583389852464987), ('城市', 0.4604600466140951), ('公益活动', 0.42613597541935094), ('全国', 0.3173819304575886), ('年度', 0.28499260676411087)]\n",
      "------------------------------\n",
      "Lsi:\n",
      "['中国', '中华', '爱心', '项目', '基金会', '社会', '城市', '公益活动', '全国', '年度']\n",
      "------------------------------\n",
      "[('中国', 1.0197322743315775), ('中华', 0.985991726468228), ('爱心', 0.8341924835701112), ('基金会', 0.6732561528476236), ('项目', 0.6637376085633867), ('社会', 0.6601663563325462), ('城市', 0.4542197366005631), ('公益活动', 0.4312422739683259), ('全国', 0.3101277260781903), ('国家', 0.2870316266024203)]\n",
      "------------------------------\n",
      "Lda:\n",
      "['中国', '中华', '爱心', '基金会', '项目', '社会', '城市', '公益活动', '全国', '国家']\n"
     ]
    }
   ],
   "source": [
    "keywords = extract_keyword(text, mode='lsi')\n",
    "print('Lsi:')\n",
    "print(keywords)\n",
    "\n",
    "keywords = extract_keyword(text, mode='lda')\n",
    "print('Lda:')\n",
    "print(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
