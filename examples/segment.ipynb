{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 规则分词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Maximum Match Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['研究生', '命', '的', '起源']\n"
     ]
    }
   ],
   "source": [
    "dictionary = {'研究', '研究生', '生命', '命', '的', '起源'}\n",
    "def tokenize_mm(document):  \n",
    "    max_length = max([len(word) for word in dictionary])\n",
    "    start = 0\n",
    "    words = list()\n",
    "    while start < len(document):\n",
    "        length = max_length\n",
    "        while length >= 1:\n",
    "            if s[start: start + length] in dictionary or length == 1:\n",
    "                words.append(s[start: start + length])\n",
    "                start += length\n",
    "                break\n",
    "            else:\n",
    "                length -= 1\n",
    "    return words\n",
    "    \n",
    "    \n",
    "s = '研究生命的起源'\n",
    "print(tokenize_mm(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Reverse Maximum Match Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['研究', '生命', '的', '起源']\n"
     ]
    }
   ],
   "source": [
    "dictionary = {'研究', '研究生', '生命', '命', '的', '起源'}     \n",
    "def tokenize_rmm(document):\n",
    "    max_length = max([len(word) for word in dictionary])\n",
    "    end = len(document)\n",
    "    words = list()\n",
    "    while end > 0:\n",
    "        length = max_length\n",
    "        while length >= 1:\n",
    "            if s[end - length: end] in dictionary or length == 1:\n",
    "                words.append(s[end - length: end])\n",
    "                end -= length\n",
    "                break\n",
    "            else:\n",
    "                length -= 1\n",
    "    words = list(reversed(words))\n",
    "    return words\n",
    "    \n",
    "    \n",
    "s = '研究生命的起源'\n",
    "print(tokenize_rmm(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Bi-direction Matching Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['研究', '生命', '的', '起源']\n"
     ]
    }
   ],
   "source": [
    "def tokenize_bd(document):\n",
    "    words_mm = tokenize_mm(document)\n",
    "    words_rmm = tokenize_rmm(document)\n",
    "    if len(words_mm) != len(words_rmm):\n",
    "        return words_mm if len(words_mm) < len(words_rmm) else words_rmm\n",
    "    elif words_mm == words_rmm:\n",
    "        return words_mm\n",
    "    else:\n",
    "        single_mm = sum([1 for word in words_mm if len(word) == 1])\n",
    "        single_rmm = sum([1 for word in words_rmm if len(word) == 1])\n",
    "        return words_mm if single_mm < single_rmm else words_rmm\n",
    "\n",
    "s = '研究生命的起源'\n",
    "print(tokenize_bd(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 统计分词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Hidden Markov Model (HMM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 给定句子$\\lambda$后某组状态$o$的概率可以表示为$ \\text{P}(o | \\lambda) $, 根据贝叶斯定理,得到$ P(o|\\lambda)  = \\dfrac{P(\\lambda | o) P(o)}{P(\\lambda)} $. 由于给定的句子固定,所以如果只是找最大的概率的话,$ P(\\lambda) $为常数可以不考虑,只需要找到$ P(\\lambda | o) P(o) $最大的$o$就可以了\n",
    "        \n",
    "\n",
    "2. 计算$ P(\\lambda | o)P(o) $的意思就是说给定状态的组合$o$,计算得到这个句子$\\lambda$的概率.假设这个状态组合是$o_1 o_2 ... o_T$,而句子是$\\lambda_1 \\lambda_2 ... \\lambda_T$        \n",
    "\n",
    "3. 对于$ P(\\lambda | o) $,我们采取最简单的每个状态独立的假设,所以$ P(\\lambda | o) = P(\\lambda_1 | o_1) P(\\lambda_2 | o_2) ... P(\\lambda_T | o_T) $; 对于$ P(o) $,我们采取2-gram,所以$ P(o) = P(o_1) P(o_2 | o_1) P(o_3 | o_2) ... P(o_T | o_{T-1}) $\n",
    "\n",
    "4. 综上所述, $ P(\\lambda|o) = P(\\lambda_1 | o_1) P(o_1) P(\\lambda_2 | o_2) P(o_2 | o_1) P(\\lambda_3 | o_3) P(o_3 | o_2) ... P(\\lambda_T | o_T) P(o_T | o_{T-1}) $\n",
    "\n",
    "5. 根据Viterbi算法,我们可以通过这种方法找到最大的概率. 假设timestep=t, 那么$ P( o_1 o_2 ... o_t |\\lambda_1 \\lambda_2 ... \\lambda_t) = max( P( o_1 o_2 ... o_{t-1} |\\lambda_1 \\lambda_2 ... \\lambda_{t-1} ) P(o_t | o_{t-1}), o_{t-1} \\in ['B', 'M', 'E', 'S'] ) P(\\lambda_t | o_t)  $, 而初始情况是:\n",
    "$ P(o_1|\\lambda_1) = P(o_1) P(\\lambda_1 | o_1) $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0010421077785925357\n",
      "0.004205160552198598\n",
      "['这是', '一个', '非常', '棒', '的', '方案', '！']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "class HMM(object):\n",
    "    def __init__(self, model_filename, corpus_filename):\n",
    "        self.states = ['B', 'M', 'E', 'S']\n",
    "        self.model_filename = model_filename\n",
    "        if os.path.exists(self.model_filename):\n",
    "            with open(model_filename, 'rb') as f:\n",
    "                self.init_prob = pickle.load(f)\n",
    "                self.trans_prob = pickle.load(f)\n",
    "                self.emit_prob = pickle.load(f)\n",
    "        else:\n",
    "            self.init_prob = dict()\n",
    "            self.trans_prob = dict()\n",
    "            self.emit_prob = dict()\n",
    "            for state in self.states:\n",
    "                self.init_prob[state] = 0.0\n",
    "                self.trans_prob[state] = {state1: 0.0 for state1 in self.states}\n",
    "                self.emit_prob[state] = dict()\n",
    "            _train(corpus_filename)\n",
    "    \n",
    "    def _train(self, corpus_filename):\n",
    "        def create_states(word):\n",
    "            states = list()\n",
    "            if len(word) == 1:\n",
    "                states.append('S')\n",
    "            elif len(word) > 1:\n",
    "                states = ['B'] + ['M'] * (len(word) - 2) + ['E']\n",
    "            return states\n",
    "            \n",
    "        num_non_empty_lines = 0\n",
    "        with open(corpus_filename, encoding='utf8') as f:\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "\n",
    "                num_non_empty_lines += 1\n",
    "                \n",
    "                words = line.split()\n",
    "                line_states = list()\n",
    "                chs = list()\n",
    "                for word in words:\n",
    "                    line_states.extend(create_states(word))\n",
    "                    for ch in word:\n",
    "                        chs.extend(ch)\n",
    "                \n",
    "                # update init_prob, trans_prob, emit_prob\n",
    "                self.init_prob[line_states[0]] += 1\n",
    "                for index in range(len(line_states)):\n",
    "                    if index != 0:\n",
    "                        self.trans_prob[line_states[index - 1]][line_states[index]] += 1\n",
    "                    self.emit_prob[line_states[index]][chs[index]] = self.emit_prob[line_states[index]].get(chs[index], 0) + 1.0\n",
    "        \n",
    "        # normalize\n",
    "        self.init_prob = {state: count / num_non_empty_lines for state, count in self.init_prob.items()}\n",
    "        self.trans_prob = {state: {dest_state: count / sum(value.values()) for dest_state, count in value.items()} for state, value in self.trans_prob.items()}\n",
    "        self.emit_prob = {state: {word: (prob + 1) / sum(word_prob.values()) for word, prob in word_prob.items()} for state, word_prob in self.emit_prob.items()}\n",
    "        \n",
    "        # dump\n",
    "        with open(self.model_filename, 'wb') as f:\n",
    "            pickle.dump(self.init_prob, f)\n",
    "            pickle.dump(self.trans_prob, f)\n",
    "            pickle.dump(self.emit_prob, f)\n",
    "    \n",
    "    def viterbi(self, document):\n",
    "        states = {} # states['E']的'E'表示这条路径的终点是'E',并且是终点是'E'的所有路径中的最优路径\n",
    "        path = [{}]\n",
    "        # init path\n",
    "        for state in self.states:\n",
    "            path[0][state] = self.init_prob[state] * self.emit_prob[state].get(document[0], 0)\n",
    "            states[state] = [state]\n",
    "        # go\n",
    "        for i in range(1, len(document)):\n",
    "            unknown = True\n",
    "            for state in self.states:\n",
    "                unknown = unknown and document[i] not in self.emit_prob[state].keys()\n",
    "            \n",
    "            tmp_states = {}\n",
    "            \n",
    "            path.append({})\n",
    "            for state in self.states:\n",
    "                emit_p = self.emit_prob[state].get(document[i], 0) if not unknown else 1.0\n",
    "                path[i][state], previous_state = max([(path[i - 1][previous_state] * self.trans_prob[previous_state][state] * emit_p, previous_state) for previous_state in self.states])\n",
    "                \n",
    "                tmp_states[state] = states[previous_state] + [state]\n",
    "            states = tmp_states\n",
    "                \n",
    "        # select the best path\n",
    "        if self.emit_prob['M'].get(document[-1], 0) > self.emit_prob['S'].get(document[-1], 0):\n",
    "            (prob, state) = max([(path[len(document) - 1][state], state) for state in ['E', 'M']])\n",
    "        else:\n",
    "            (prob, state) = max([(path[len(document) - 1][state], state) for state in self.states])\n",
    "        \n",
    "            \n",
    "        return prob, states[state]\n",
    "    \n",
    "    def tokenize(self, document):\n",
    "        prob, states = self.viterbi(document)\n",
    "        words = list()\n",
    "        index = 0\n",
    "        while index < len(states):\n",
    "        #for index, state in enumerate(states):\n",
    "            if states[index] == 'B':\n",
    "                end = states[index:].index('E') + index + 1\n",
    "                words.append(document[index: end])\n",
    "                index = end\n",
    "            elif states[index] == 'S':\n",
    "                words.append(document[index])\n",
    "                index += 1\n",
    "        return words\n",
    "        \n",
    "model_filename = 'data/hmm.pkl'\n",
    "corpus_filename = 'data/trainCorpus.txt_utf8'\n",
    "hmm = HMM(model_filename, corpus_filename)\n",
    "\n",
    "#print(hmm.emit_prob)\n",
    "print(hmm.emit_prob['E']['案'])\n",
    "print(hmm.emit_prob['B']['方'])\n",
    "\n",
    "document = '这是一个非常棒的方案！'\n",
    "words = hmm.tokenize(document)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
